{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXz4yDd6qhQAtdEgcKuYC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaLuWi/guayas_store_project/blob/main/Masterschool_Week3_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1p8--x1cS_B",
        "outputId": "44bb9d06-7648-42e3-bb35-863fd37e0a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Note: train file is very large.\n",
        "# To make sure we fit RAM avaible on Google-Colab, we will load only part of the file\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Time Series/datasets/df_train2.csv') #this is the dataset after filtering out for the Guayas region and top3 familys\n",
        "df_stores = pd.read_csv('/content/drive/MyDrive/Time Series/datasets/stores.csv')\n",
        "df_items = pd.read_csv('/content/drive/MyDrive/Time Series/datasets/items.csv')\n",
        "df_transactions = pd.read_csv('/content/drive/MyDrive/Time Series/datasets/transactions.csv')\n",
        "df_oil = pd.read_csv('/content/drive/MyDrive/Time Series/datasets/oil.csv')\n",
        "df_holidays_events = pd.read_csv('/content/drive/MyDrive/Time Series/datasets/holidays_events.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking the datasets structure**"
      ],
      "metadata": {
        "id": "5gqrxhw7cj9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "ItVnPVX7eeaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['date'].describe()"
      ],
      "metadata": {
        "id": "_dvQC4I8WqJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "5nAqs4HPhTrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stores.head()"
      ],
      "metadata": {
        "id": "GhS8NDuPuea0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_holidays_events.head()"
      ],
      "metadata": {
        "id": "Lz8m0JhTujc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_items.head()"
      ],
      "metadata": {
        "id": "V-luayg0uoCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oil.head()"
      ],
      "metadata": {
        "id": "z7VuAtyVur5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oil.describe()"
      ],
      "metadata": {
        "id": "LJC7F0eLL8cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transactions.head()"
      ],
      "metadata": {
        "id": "atE1ywaLuyRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_date = '2014-01-01'\n",
        "train = df_train[df_train['date'] < split_date]\n",
        "test = df_train[df_train['date'] > split_date]"
      ],
      "metadata": {
        "id": "RhB7diTO78CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "r_JR126n5g96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "qbNppcqi5iqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "lvDcOnQEzxrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply the Model"
      ],
      "metadata": {
        "id": "eZenbgjTIMAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the librariers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "metadata": {
        "id": "PAxn0vzm-Cck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute total sales per item\n",
        "item_totals = df_train.groupby('item_nbr')['unit_sales'].sum().sort_values(ascending=False)\n",
        "# Choose top 150 items\n",
        "top_items = item_totals.head(150).index.tolist()\n",
        "# Filter df_train to include only top 150 items\n",
        "df_train = df_train[df_train['item_nbr'].isin(top_items)]\n",
        "#I go with the most impactful 10% of items"
      ],
      "metadata": {
        "id": "-eneL6VAUa0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dates to datetime\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "\n",
        "# Optimize data types in df_train\n",
        "df_train = df_train.astype({\n",
        "    'store_nbr': 'category',\n",
        "    'item_nbr': 'category',\n",
        "    'onpromotion': 'bool',\n",
        "    'year': 'int16',\n",
        "    'month': 'int8',\n",
        "    'day': 'int8',\n",
        "    'day_of_week': 'int8',\n",
        "    'unit_sales_7d_avg': 'float32',\n",
        "    'lag_1': 'float32',\n",
        "    'lag_7': 'float32',\n",
        "    'lag_14': 'float32',\n",
        "    'lag_30': 'float32',\n",
        "    'roll7_std': 'float32',\n",
        "    'pct_chg_7d': 'float32',\n",
        "    'unit_sales': 'float32'\n",
        "})\n",
        "\n",
        "# Merge datasets\n",
        "df_train = df_train.merge(df_stores, on='store_nbr', how='left')\n",
        "df_train = df_train.merge(df_items, on='item_nbr', how='left')\n",
        "\n",
        "# Add is_weekend feature\n",
        "df_train['is_weekend'] = df_train['day_of_week'].isin([5, 6]).astype('bool')\n",
        "\n",
        "# Ensure categorical columns\n",
        "categorical_cols = ['store_nbr', 'item_nbr', 'family', 'city', 'state', 'type_x', 'class']\n",
        "for col in categorical_cols:\n",
        "    if col in df_train.columns:\n",
        "        df_train[col] = df_train[col].astype('category')\n",
        "\n",
        "# Rename type_x to avoid confusion\n",
        "df_train = df_train.rename(columns={'type_x': 'store_type'})"
      ],
      "metadata": {
        "id": "J0yzzdpCUc54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split prep\n",
        "split_date = '2014-01-01'\n",
        "train = df_train[df_train['date'] < split_date]\n",
        "test = df_train[df_train['date'] >= split_date]"
      ],
      "metadata": {
        "id": "1p2q-DyHVF15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\n",
        "    'store_nbr', 'item_nbr', 'onpromotion', 'year', 'month', 'day', 'day_of_week',\n",
        "    'unit_sales_7d_avg', 'lag_1', 'lag_7', 'lag_14', 'lag_30', 'roll7_std',\n",
        "    'pct_chg_7d', 'is_weekend', 'city', 'state', 'store_type',\n",
        "    'cluster', 'family', 'class', 'perishable'\n",
        "]\n",
        "# Ensure only existing columns are included\n",
        "feature_cols = [col for col in feature_cols if col in df_train.columns]\n",
        "X_train = train[feature_cols]\n",
        "y_train = train['unit_sales']\n",
        "X_test = test[feature_cols]\n",
        "y_test = test['unit_sales']"
      ],
      "metadata": {
        "id": "h3dCpW_BVG87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'XGBoost': {\n",
        "        'model': xgb.XGBRegressor(objective='reg:squarederror', enable_categorical=True),\n",
        "        'param_grid': {\n",
        "            'eta': [0.01, 0.1, 0.3],\n",
        "            'max_depth': [3, 5, 7, 10],\n",
        "            'subsample': [0.6, 0.8, 1.0],\n",
        "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "            'n_estimators': [100, 200, 500]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "x1TYsIp-VMM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "best_models = {}\n",
        "best_scores = {}\n",
        "for model_name, model_info in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model_info['model'],\n",
        "        param_distributions=model_info['param_grid'],\n",
        "        cv=tscv,\n",
        "        n_iter=20,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        verbose=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_models[model_name] = random_search.best_estimator_\n",
        "    best_scores[model_name] = random_search.best_score_\n",
        "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
        "    print(f\"Best CV score (MSE) for {model_name}: {-random_search.best_score_}\")\n",
        "\n",
        "best_model_name = max(best_scores, key=best_scores.get)\n",
        "best_model = best_models[best_model_name]\n",
        "print(f\"Best model: {best_model_name} with CV score (MSE): {-best_scores[best_model_name]}\")"
      ],
      "metadata": {
        "id": "C_8wNcVUVS4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "print(f\"Evaluation metrics for {best_model_name}:\")\n",
        "print(\"R2:\", r2_score(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "fOgLNmY2VUGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 30\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(test['date'][:i], y_test.values[:i], label='Actual Sales', marker='o')\n",
        "plt.plot(test['date'][:i], y_pred[:i], label='Predicted Sales', marker='x', color='red')\n",
        "plt.title(f'Actual vs Predicted Sales using {best_model_name}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Unit Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/MyDrive/demand_forecast_plot.png')"
      ],
      "metadata": {
        "id": "88Gsi-wqVVcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_future_data(store_nbr, item_nbr, start_date, days_ahead, df_train, df_stores, df_items):\n",
        "    future_dates = pd.date_range(start=start_date, periods=days_ahead, freq='D')\n",
        "    future_df = pd.DataFrame({\n",
        "        'date': future_dates,\n",
        "        'store_nbr': store_nbr,\n",
        "        'item_nbr': item_nbr,\n",
        "        'onpromotion': False,\n",
        "        'year': future_dates.year,\n",
        "        'month': future_dates.month,\n",
        "        'day': future_dates.day,\n",
        "        'day_of_week': future_dates.dayofweek\n",
        "    })\n",
        "\n",
        "    # Merge store and item info\n",
        "    future_df = future_df.merge(df_stores, on='store_nbr', how='left')\n",
        "    future_df = future_df.merge(df_items, on='item_nbr', how='left')\n",
        "\n",
        "    # Add lag and rolling features\n",
        "    last_data = df_train[(df_train['store_nbr'] == store_nbr) & (df_train['item_nbr'] == item_nbr)].tail(30)\n",
        "    future_df['lag_1'] = np.pad(last_data['unit_sales'].values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "    future_df['lag_7'] = np.pad(last_data['unit_sales'].shift(6).fillna(method='bfill').values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "    future_df['lag_14'] = np.pad(last_data['unit_sales'].shift(13).fillna(method='bfill').values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "    future_df['lag_30'] = np.pad(last_data['unit_sales'].shift(29).fillna(method='bfill').values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "    future_df['unit_sales_7d_avg'] = np.pad(last_data['unit_sales'].rolling(window=7).mean().fillna(method='bfill').values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "    future_df['roll7_std'] = np.pad(last_data['unit_sales'].rolling(window=7).std().fillna(method='bfill').values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "    future_df['pct_chg_7d'] = np.pad(last_data['unit_sales'].pct_change(periods=7).fillna(0).values, (0, days_ahead - len(last_data)), mode='edge')[:days_ahead]\n",
        "\n",
        "    # Add is_weekend\n",
        "    future_df['is_weekend'] = future_df['day_of_week'].isin([5, 6]).astype('bool')\n",
        "\n",
        "    # Ensure categorical columns\n",
        "    categorical_cols = ['store_nbr', 'item_nbr', 'family', 'city', 'state', 'store_type', 'class']\n",
        "    for col in categorical_cols:\n",
        "        if col in future_df.columns:\n",
        "            future_df[col] = future_df[col].astype('category')\n",
        "\n",
        "    # Ensure feature order matches training\n",
        "    return future_df[feature_cols]"
      ],
      "metadata": {
        "id": "Ab6nui53VWe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_path = '/content/drive/MyDrive/models/best_demand_forecast_model.pkl'\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "print(f\"Best model saved as pickle file at: {pickle_path}\")"
      ],
      "metadata": {
        "id": "1zLp5JTVVXvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "MiSM2kFei1D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "model_path = os.path.join(model_dir, f'{best_model_name}_forecast_model.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "print(f\"Model saved at: {model_path}\")\n"
      ],
      "metadata": {
        "id": "RDrDUk9wiz-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}